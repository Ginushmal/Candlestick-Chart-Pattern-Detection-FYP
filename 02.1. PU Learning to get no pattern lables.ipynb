{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('Datasets/VanilaDataset/test_patterns_with_symbols.csv')\n",
    "train_dataset = pd.read_csv('Datasets/VanilaDataset/train_patterns_with_symbols.csv')\n",
    "\n",
    "file_path = \"Datasets/VanilaDataset\"\n",
    "train_dataset_processed = pd.read_csv(file_path + \"/trainDataset_w_aug.csv\", index_col=[0, 1])\n",
    "test_dataset_processed = pd.read_csv(file_path + \"/testDataset_w_aug.csv\" , index_col=[0, 1])\n",
    "\n",
    "# combine the datasets\n",
    "dataset = pd.concat([train_dataset, test_dataset])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by Start \n",
    "dataset = dataset.sort_values(by='Start')\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "# coonvert Start and End to datetime\n",
    "dataset['Start'] = pd.to_datetime(dataset['Start'])\n",
    "dataset['End'] = pd.to_datetime(dataset['End'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the unique symbols\n",
    "symbols = dataset['Symbol'].unique()\n",
    "symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataset with Lable less segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_of_unlabled_seg =10000\n",
    "# Set a common random seed\n",
    "SEED = 69\n",
    "np.random.seed(SEED)\n",
    "\n",
    "no_pattern_rows = []\n",
    "\n",
    "with tqdm(total=num_of_unlabled_seg, desc=\"Processing Segments\") as pbar:\n",
    "    while num_of_unlabled_seg > 0:\n",
    "        # get a random symbol from the list of symbols\n",
    "        random_symbol = np.random.choice(symbols)\n",
    "        # get the dataset for the random symbol\n",
    "        symbol_dataset = dataset[dataset['Symbol'] == random_symbol]\n",
    "        symbol_dataset = symbol_dataset.sort_values(by='Start')\n",
    "        symbol_dataset.reset_index(drop=True, inplace=True)\n",
    "        if(len(symbol_dataset)>1):\n",
    "            # get a random index from the symbol dataset\n",
    "            random_index = np.random.randint(0, len(symbol_dataset)-1)\n",
    "            # get the random segment\n",
    "            random_start_segment = symbol_dataset.iloc[random_index]\n",
    "            random_end_segment = symbol_dataset.iloc[random_index + 1]\n",
    "            \n",
    "            # get the date range between the End date of the start segment and the Start date of the end segment\n",
    "            num_of_dates = random_end_segment['Start'] - random_start_segment['End']\n",
    "            num_of_dates = num_of_dates.days\n",
    "            \n",
    "            if num_of_dates > 10:\n",
    "                min_start_date = random_start_segment['End']\n",
    "                max_end_date = random_end_segment['Start']\n",
    "                \n",
    "                # print(\"min_start_date: \", min_start_date , \"max_end_date: \", max_end_date,\"date len\",(max_end_date - min_start_date).days)\n",
    "                \n",
    "                # get a random start and end date between the min and max date\n",
    "                random_start_date_i = np.random.randint(0, num_of_dates-10)\n",
    "                random_start_date = min_start_date + pd.DateOffset(days=random_start_date_i)\n",
    "                \n",
    "                if((num_of_dates+1) -(random_start_date_i) > 100 ):\n",
    "                    random_end_right_margin = random_start_date_i +100\n",
    "                else:\n",
    "                    random_end_right_margin = num_of_dates+1\n",
    "                \n",
    "                random_end_date_i = np.random.randint(random_start_date_i +10, random_end_right_margin)\n",
    "                random_end_date = min_start_date + pd.DateOffset(days=random_end_date_i)\n",
    "                \n",
    "                # print(\"random_start_date: \", random_start_date , \"random_end_date: \", random_end_date,\"date len\",(random_end_date - random_start_date).days)\n",
    "\n",
    "                # check if there is an segment already in the no_pattern_rows list that intersects with the random segment\n",
    "                # print('start checking ')\n",
    "                intersect = False\n",
    "                for row in no_pattern_rows:\n",
    "                    if (random_symbol == row['Symbol']) and ((random_start_date >= row['Start'] and random_start_date <= row['End']) or (random_end_date >= row['Start'] and random_end_date <= row['End'])):\n",
    "                        intersect_len = (min(random_end_date, row['End']) - max(random_start_date, row['Start'])).days\n",
    "                        intersect_percent = intersect_len / (random_end_date - random_start_date).days\n",
    "                        if intersect_percent > 0.2:\n",
    "                            intersect = True\n",
    "                            # print('Intersecting segment found in symbol: ', random_symbol,\" and \",row['Symbol'], 'date range ', random_start_date, random_end_date ,' and ', row['Start'], row['End'])\n",
    "                            break\n",
    "                \n",
    "                # print('end checking ')\n",
    "                \n",
    "                if not intersect:\n",
    "                    row = {\n",
    "                        'Symbol': random_symbol,\n",
    "                        'Chart Pattern': 'No Pattern',\n",
    "                        'BullishBearish' : random_start_segment['BullishBearish'],\n",
    "                        'Start': random_start_date,\n",
    "                        'End': random_end_date, \n",
    "                        'Industry' : random_start_segment['Industry'],   \n",
    "                        'Pattern_Length' : (random_end_date - random_start_date).days\n",
    "                    }\n",
    "                    \n",
    "                    no_pattern_rows.append(row)\n",
    "                    num_of_unlabled_seg -= 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                \n",
    "\n",
    "no_pattern_df = pd.DataFrame(no_pattern_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the no_pattern_df Pattern_Length colum with correct values\n",
    "# loop through each row in the no_pattern_df\n",
    "for index, row in no_pattern_df.iterrows():\n",
    "    symbol = row['Symbol']\n",
    "    start_date = row['Start']\n",
    "    end_date = row['End']\n",
    "    # set to date time \n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    # read the OHLC data for the symbol from file\n",
    "    ohlc_data = pd.read_csv(f'Datasets/OHLC data/{symbol}.csv')\n",
    "    # set the Date column to datetime\n",
    "    ohlc_data['Date'] = pd.to_datetime(ohlc_data['Date'])\n",
    "    ohlc_data['Date'] = ohlc_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "    # get the ohlc data between the start and end date\n",
    "    ohlc_data = ohlc_data[(ohlc_data['Date'] >= start_date) & (ohlc_data['Date'] <= end_date)]\n",
    "    # get the length of the ohlc data\n",
    "    pattern_length = len(ohlc_data)\n",
    "    # update the Pattern_Length column with the correct value\n",
    "    no_pattern_df.at[index, 'Pattern_Length'] = pattern_length\n",
    "\n",
    "no_pattern_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pattern_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save no_pattern_df to csv \n",
    "no_pattern_df.to_csv(\"Datasets/VanilaDataset/no_pattern_10000_df.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.formatAndPreprocess import dataset_format\n",
    "\n",
    "#  create formatted data set for the train and test data\n",
    "no_pattern_dataset,instance_index_mapping = dataset_format(no_pattern_df,give_instance_index_mapping=True)\n",
    "no_pattern_dataset , instance_index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_processed_copy = train_dataset_processed.copy()\n",
    "# make a new column with the same values as the Instance index(Level 0) values\n",
    "train_dataset_processed_copy['Instance'] = train_dataset_processed_copy.index.get_level_values(0)\n",
    "train_dataset_processed_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Instance_Lits = [2,1,0,6406]\n",
    "# get the data set where the Instance is in the list\n",
    "train_dataset_processed_copy[train_dataset_processed_copy['Instance'].isin(Instance_Lits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PU Bagging Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "from xgboost import XGBClassifier\n",
    "from utils.FixedLengthTransformer import FixedLengthTransformer\n",
    "\n",
    "def train_mini_rcket(X_train,y_train) :\n",
    "    fl = FixedLengthTransformer(fixed_length=100, fill_value=0)\n",
    "    mini_rocket = MiniRocketMultivariate(num_kernels=10000)\n",
    "    xgbmr = XGBClassifier(\n",
    "        eval_metric='mlogloss', \n",
    "        n_estimators=100,\n",
    "    )\n",
    "\n",
    "    clf_mini_rocket_xgb = make_pipeline(\n",
    "        fl,\n",
    "        mini_rocket,\n",
    "        xgbmr\n",
    "    )\n",
    "    clf_mini_rocket_xgb.fit(X_train, y_train)\n",
    "\n",
    "    return clf_mini_rocket_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# - Create a training set by combining au positive data points With a random \n",
    "#   sample from the unlabeled points. With replacement.\n",
    "# - Build a classifier from this \"bootstrap\" sample. treating positive and unlabeled \n",
    "#   data points as positives and negatives. respectively,\n",
    "# - Apply the classifier to whatever the unlabeled data points were not included in the \n",
    "#   random sample - hereafter called OOB (Out of Bag) points â€” and record their scores.\n",
    "# - Repeat the three Steps above many times and finally assign to each point the average \n",
    "#   Of OOB scores it has received.\n",
    "\n",
    "# get the unique value list of level 0 index Instance\n",
    "unique_unlabeled_instance = no_pattern_dataset.index.get_level_values(0).unique()\n",
    "# for each data point , keep track of how many times it has ben OOB and the sum of the probabilities assigned to it\n",
    "oob_no_pattern_df = pd.DataFrame(index=unique_unlabeled_instance, columns=['oob_count', 'oob_sum'])\n",
    "oob_no_pattern_df['oob_count'] = 0\n",
    "oob_no_pattern_df['oob_sum'] = 0\n",
    "\n",
    "n_estimators = 250\n",
    "\n",
    "\n",
    "# Create a tqdm object and assign it to a variable\n",
    "progress_bar = tqdm(range(n_estimators), desc=\"Training Progress\", unit=\"iteration\")\n",
    "\n",
    "for i in progress_bar:\n",
    "    # get a 20% sample of Instance index from the instance_list_unlabled with replacement \n",
    "    sample_train_instances = np.random.choice(unique_unlabeled_instance, size=int(0.2 * len(unique_unlabeled_instance)), replace=True)\n",
    "    sample_test_instances = unique_unlabeled_instance [~np.isin(unique_unlabeled_instance, sample_train_instances)]\n",
    "    # get the data set where the Instance is in the list\n",
    "    sample_train = no_pattern_dataset[no_pattern_dataset.index.get_level_values(0).isin(sample_train_instances)]\n",
    "    sample_test = no_pattern_dataset[no_pattern_dataset.index.get_level_values(0).isin(sample_test_instances)]\n",
    "\n",
    "    sample_test = sample_test.copy()  # Make an explicit copy\n",
    "    sample_test['Instance'] = sample_test.index.get_level_values(0)\n",
    "\n",
    "\n",
    "    # Extract Instance and Time indices from sample_test\n",
    "    instance_idx = sample_test.index.get_level_values(0)\n",
    "    time_idx = sample_test.index.get_level_values(1)\n",
    "\n",
    "    # Create new Instance index starting from max_instance + 1\n",
    "    new_instance_idx = pd.factorize(instance_idx)[0]  \n",
    "\n",
    "    # Reconstruct the MultiIndex with updated Instance indices\n",
    "    sample_test.index = pd.MultiIndex.from_arrays([new_instance_idx, time_idx], names=[\"Instance\", \"Time\"])\n",
    "\n",
    "\n",
    "    # create a mapping dictionary from the old Instance index to the new Instance index\n",
    "    test_inst_mapping_dict = dict(zip(new_instance_idx,instance_idx))\n",
    "    # print(test_inst_mapping_dict)\n",
    "\n",
    "\n",
    "    # print(\"Sampek test :\",sample_test)\n",
    "    # print(sample_train)\n",
    "\n",
    "    # get the  largest Instance index in the train_dataset_processed\n",
    "    max_instance = train_dataset_processed.index.get_level_values(0).max()\n",
    "\n",
    "    # Extract Instance and Time indices from sample_train\n",
    "    instance_idx = sample_train.index.get_level_values(0)\n",
    "    time_idx = sample_train.index.get_level_values(1)\n",
    "\n",
    "    # Create new Instance index starting from max_instance + 1\n",
    "    new_instance_idx = pd.factorize(instance_idx)[0] + max_instance + 1  # Ensure unique new indices\n",
    "\n",
    "    # Reconstruct the MultiIndex with updated Instance indices\n",
    "    sample_train.index = pd.MultiIndex.from_arrays([new_instance_idx, time_idx], names=[\"Instance\", \"Time\"])\n",
    "\n",
    "\n",
    "\n",
    "    # print(sample_train.index.get_level_values(0).unique())\n",
    "    # print(len(sample_train.index.get_level_values(0).unique()))\n",
    "    # print(sample_train)\n",
    "\n",
    "    # concatanate the sample with the train_dataset_processed data\n",
    "    train_data = pd.concat([train_dataset_processed, sample_train])\n",
    "    X_train = train_data.drop(columns=['Pattern'])\n",
    "    y_train = train_data.groupby(level=0)['Pattern'].first().to_frame()\n",
    "    X_test = test_dataset_processed.drop(columns=['Pattern'])\n",
    "    y_test = test_dataset_processed.groupby(level=0)['Pattern'].first().to_frame()\n",
    "\n",
    "    mini_rocket_model = train_mini_rcket(X_train,y_train)\n",
    "\n",
    "    # # get the probabilities of the test data\n",
    "    # y_test_prob = mini_rocket_model.predict_proba(X_test)\n",
    "    # y_test_pred = y_test_prob.argmax(axis=1)  \n",
    "    # test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # # print(\"Test accuracy for the \",i,\"th model: \", test_accuracy)\n",
    "    \n",
    "    # # Update the postfix with the previous accuracy\n",
    "    # progress_bar.set_postfix({\"Prev Accuracy\": f\"{test_accuracy:.4f}\" if test_accuracy is not None else \"N/A\"})\n",
    "\n",
    "    # get the probabilities of the sample test data\n",
    "    y_sample_test_prob = mini_rocket_model.predict_proba(sample_test)\n",
    "\n",
    "    for j in range(0, len(y_sample_test_prob)):\n",
    "        correct_instance = test_inst_mapping_dict[j]\n",
    "        # print(y_sample_test_prob[j][7])\n",
    "        oob_no_pattern_df.loc[correct_instance, 'oob_count'] += 1\n",
    "        oob_no_pattern_df.loc[correct_instance, 'oob_sum'] += y_sample_test_prob[j][7]\n",
    "oob_no_pattern_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the row print constrain in pandas \n",
    "pd.reset_option('display.max_rows')\n",
    "oob_no_pattern_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save oob_no_pattern_df to a csv file\n",
    "oob_no_pattern_df.to_csv('Datasets/VanilaDataset/oob_no_pattern_stats_df.csv')\n",
    "# save the dictionary to a csv \n",
    "instance_index_mapping_df = pd.DataFrame(instance_index_mapping.items(), columns=['Instance', 'Index'])\n",
    "instance_index_mapping_df.to_csv('Datasets/VanilaDataset/instance_index_mapping_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary to a csv \n",
    "instance_index_mapping_df = pd.DataFrame(instance_index_mapping.items(), columns=['Instance', 'Index'])\n",
    "instance_index_mapping_df.to_csv('Datasets/VanilaDataset/instance_index_mapping_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv dataset\n",
    "oob_no_pattern_df = pd.read_csv('Datasets/VanilaDataset/oob_no_pattern_stats_df.csv', index_col=0)\n",
    "oob_no_pattern_df[\"Avg_Prob\"] = oob_no_pattern_df['oob_sum'] / oob_no_pattern_df['oob_count']\n",
    "oob_no_pattern_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by the average probability column\n",
    "oob_no_pattern_df = oob_no_pattern_df.sort_values(by='Avg_Prob', ascending=False)\n",
    "oob_no_pattern_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the DataFrame by 'Avg_Prob' in descending order\n",
    "sorted_df = oob_no_pattern_df.sort_values(by='Avg_Prob', ascending=False)\n",
    "\n",
    "# sorted_df equals the data where the 'Avg_Prob' is higher than 0.5\n",
    "top_instances = sorted_df.head(200)\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_instances.index.astype(str), top_instances['Avg_Prob'], color='skyblue')\n",
    "plt.xlabel('Average Probability')\n",
    "plt.ylabel('Instance')\n",
    "plt.title('Sorted Average Probability of No Pattern Instances')\n",
    "\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have highest at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df[sorted_df['Avg_Prob'] > 0.08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 200 instances with the highest average probability\n",
    "top_200_instances = sorted_df.head(200)\n",
    "top_200_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mplfinance as mpf\n",
    "\n",
    "def plot_ohlc_segment(data_segment):\n",
    "    \"\"\"\n",
    "    Plots a segment of OHLC data using mplfinance.\n",
    "\n",
    "    Parameters:\n",
    "    - data_segment (pd.DataFrame): A DataFrame containing columns ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame index is datetime for mplfinance\n",
    "    data_segment = data_segment.copy()\n",
    "    data_segment.index = pd.date_range(start='2024-01-01', periods=len(data_segment), freq='D')\n",
    "\n",
    "    # Plot the candlestick chart\n",
    "    mpf.plot(data_segment, type='candle', style='charles',\n",
    "             volume=True, ylabel='Price', ylabel_lower='Volume',\n",
    "             title=\"OHLC Segment\", figsize=(10, 6))\n",
    "\n",
    "\n",
    "# Call the function to plot\n",
    "plot_ohlc_segment(no_pattern_dataset.loc[1005])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test full sets by combining data from no pattern segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from here incase of test train data change occurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('Datasets/VanilaDataset/test_patterns_with_symbols.csv')\n",
    "train_dataset = pd.read_csv('Datasets/VanilaDataset/train_patterns_with_symbols.csv')\n",
    "\n",
    "file_path = \"Datasets/VanilaDataset\"\n",
    "train_dataset_processed = pd.read_csv(file_path + \"/trainDataset_w_aug.csv\", index_col=[0, 1])\n",
    "test_dataset_processed = pd.read_csv(file_path + \"/testDataset_w_aug.csv\" , index_col=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv dataset\n",
    "oob_no_pattern_df = pd.read_csv('Datasets/VanilaDataset/oob_no_pattern_stats_df.csv', index_col=0)\n",
    "oob_no_pattern_df[\"Avg_Prob\"] = oob_no_pattern_df['oob_sum'] / oob_no_pattern_df['oob_count']\n",
    "sorted_df = oob_no_pattern_df.sort_values(by='Avg_Prob', ascending=False)\n",
    "top_200_instances = sorted_df.head(200)\n",
    "\n",
    "no_pattern_df = pd.read_csv('Datasets/VanilaDataset/no_pattern_10000_df.csv', index_col=0)\n",
    "# Read the CSV file\n",
    "instance_index_mapping_df = pd.read_csv('Datasets/VanilaDataset/instance_index_mapping_df.csv', index_col=0)\n",
    "# Convert back to dictionary\n",
    "instance_index_mapping = dict(zip(instance_index_mapping_df['Instance'], instance_index_mapping_df['Index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_200_instances = top_200_instances.index\n",
    "top_200_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random 80% of the top 200 instances for training and 20% for testing\n",
    "train_instances = np.random.choice(top_200_instances, size=int(0.8 * len(top_200_instances)), replace=False)\n",
    "test_instances = top_200_instances[~np.isin(top_200_instances, train_instances)]\n",
    "len(train_instances), len(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test and train OHLC data for no pattern instances\n",
    "train_no_pattern_ohlc = no_pattern_dataset.loc[train_instances]\n",
    "test_no_pattern_ohlc = no_pattern_dataset.loc[test_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_indices_train = [instance_index_mapping[key] for key in train_instances]  # Convert instances to original indices\n",
    "train_no_pattern_details = no_pattern_df.loc[mapped_indices_train]\n",
    "train_no_pattern_details.reset_index(drop=True, inplace=True)\n",
    "\n",
    "mapped_indices_test = [instance_index_mapping[key] for key in test_instances]  # Convert instances to original indices\n",
    "test_no_pattern_details = no_pattern_df.loc[mapped_indices_test]\n",
    "test_no_pattern_details.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_pattern_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_pattern_ohlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine in to existing datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatanate the train_no_pattern_details with the train_dataset\n",
    "full_train_no_pattern_details = pd.concat([train_dataset, train_no_pattern_details])\n",
    "full_train_no_pattern_details.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# concatanate the test_no_pattern_details with the test_dataset\n",
    "full_test_no_pattern_details = pd.concat([test_dataset, test_no_pattern_details])\n",
    "full_test_no_pattern_details.reset_index(drop=True, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def instance_index_reset(ohlc_multi_idx_df , max_instance= 0):\n",
    "    # Extract Instance and Time indices from sample_train\n",
    "    instance_idx = ohlc_multi_idx_df.index.get_level_values(0)\n",
    "    time_idx = ohlc_multi_idx_df.index.get_level_values(1)\n",
    "\n",
    "    # Create new Instance index starting from max_instance + 1\n",
    "    new_instance_idx = pd.factorize(instance_idx)[0] + max_instance + 1  # Ensure unique new indices\n",
    "\n",
    "    # Reconstruct the MultiIndex with updated Instance indices\n",
    "    ohlc_multi_idx_df.index = pd.MultiIndex.from_arrays([new_instance_idx, time_idx], names=[\"Instance\", \"Time\"])\n",
    "    \n",
    "    return ohlc_multi_idx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_pattern_ohlc = instance_index_reset(train_no_pattern_ohlc, max_instance=train_dataset_processed.index.get_level_values(0).max())\n",
    "test_no_pattern_ohlc = instance_index_reset(test_no_pattern_ohlc, max_instance=test_dataset_processed.index.get_level_values(0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatanate the train_no_pattern_ohlc with the train_dataset_processed\n",
    "full_train_no_pattern_ohlc = pd.concat([train_dataset_processed, train_no_pattern_ohlc])\n",
    "\n",
    "# concatanate the test_no_pattern_ohlc with the test_dataset_processed\n",
    "full_test_no_pattern_ohlc = pd.concat([test_dataset_processed, test_no_pattern_ohlc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Start' and 'End' columns to datetime format\n",
    "full_train_no_pattern_details['Start'] = pd.to_datetime(full_train_no_pattern_details['Start'], errors='coerce')\n",
    "full_train_no_pattern_details['End'] = pd.to_datetime(full_train_no_pattern_details['End'], errors='coerce')\n",
    "\n",
    "# Remove any time component (optional if you only want dates)\n",
    "full_train_no_pattern_details['Start'] = full_train_no_pattern_details['Start'].dt.date\n",
    "full_train_no_pattern_details['End'] = full_train_no_pattern_details['End'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Start' and 'End' columns to datetime format\n",
    "full_test_no_pattern_details['Start'] = pd.to_datetime(full_test_no_pattern_details['Start'], errors='coerce')\n",
    "full_test_no_pattern_details['End'] = pd.to_datetime(full_test_no_pattern_details['End'], errors='coerce')\n",
    "\n",
    "# Remove any time component (optional if you only want dates)\n",
    "full_test_no_pattern_details['Start'] = full_test_no_pattern_details['Start'].dt.date\n",
    "full_test_no_pattern_details['End'] = full_test_no_pattern_details['End'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"Datasets/VanilaDataset/PU results/\"\n",
    "\n",
    "# create the folder if it does not exist\n",
    "import os\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# save to a csv file\n",
    "full_train_no_pattern_details.to_csv(folder_path+'train_PU_no_pattern_details.csv')\n",
    "full_train_no_pattern_ohlc.to_csv(folder_path+'train_PU_no_pattern_ohlc.csv')\n",
    "\n",
    "full_test_no_pattern_details.to_csv(folder_path+'test_PU_no_pattern_details.csv')\n",
    "full_test_no_pattern_ohlc.to_csv(folder_path+'test_PU_no_pattern_ohlc.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
